{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск токсичных комментариев\n",
    "\n",
    "### Задача\n",
    "\n",
    "---\n",
    "\n",
    "Интернет-магазин запускает сервис, в которм пользователи могут редактировать и дополнять описания товаров. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Нужно обучить модель классифицировать комментарии на позитивные и негативные.\n",
    "\n",
    "*Условие*: метрика качества *F1* должна быть не меньше 0.75.\n",
    "\n",
    "### Данные\n",
    "\n",
    "---\n",
    "\n",
    "Набор данных с разметкой о токсичности правок:\n",
    "- `text` - текст комментария\n",
    "- `toxic` - является ли токсичным\n",
    "\n",
    "### План проекта:\n",
    "\n",
    "---\n",
    "1. Подготовка данных.\n",
    "    - Знакомство с данными. Выделение обучающей и тестовой подвыборок. Получение эмбеддингов.\n",
    "2. Обучение моделей.\n",
    "    - Логистическая регрессия, случайный лес, LightGBM, CatBoost. Тестирование лучшей модели.\n",
    "3. Вывод.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/toxic_comments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на общую информацию о датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_info(data):\n",
    "    display(data.head())\n",
    "    print()\n",
    "    print(data.info())\n",
    "    print(f\"\\nЯвных дубликатов: {data.duplicated().sum()}\")\n",
    "    print(\"\\nПодсчет классов:\")\n",
    "    print(data['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n",
      "None\n",
      "\n",
      "Явных дубликатов: 0\n",
      "\n",
      "Подсчет классов:\n",
      "toxic\n",
      "0    143106\n",
      "1     16186\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_info(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:**\n",
    "\n",
    "Датасет содержит 159 292 текстовых комментария с разметкой токсичности `toxic`. Решается задача бинарной классификации. Явных дубликатов нет. Наблюдается сильный дисбаланс классов: токсичных комментариев всего около 10%.\n",
    "\n",
    "Для обучения моделей данные нужно подготовить. Что нужно сделать:\n",
    "\n",
    "- избавиться от лишних столбцов;\n",
    "- учесть дисбаланс классов;\n",
    "- векторизовать тексты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных\n",
    "\n",
    "Удалим из датасета ненужный столбец `Unnamed: 0`, так как он дублирует индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет довольно большой, и получение эмбеддингов с помощью BERT займет довольно много времени. Поэтому ограничимся лишь частью данных. Возьмем выборку из 1000 объектов и проверим, что соотношение классов примерно то же."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    894\n",
       "1    106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = data.sample(n=1000, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "comments['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объектов положительного класса также ~10%.\n",
    "\n",
    "Разделим данные на обучающую и тестовую выборки в соотношении 8:2 с учетом стратификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры выборок:\n",
      "train - 800 - 80%\n",
      "test - 200 - 20%\n"
     ]
    }
   ],
   "source": [
    "comments_train, comments_test, y_train, y_test = train_test_split(\n",
    "    comments['text'], comments['toxic'], test_size=0.2, stratify=comments['toxic'], random_state=RANDOM_STATE)\n",
    "\n",
    "print('Размеры выборок:')\n",
    "print(f\"train - {len(comments_train)} - {len(comments_train)/len(comments['text']):.0%}\")\n",
    "print(f\"test - {len(comments_test)} - {len(comments_test)/len(comments['text']):.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эмбеддинги\n",
    "\n",
    "Векторизуем текст в эмбеддинги с помощью предобученной модели [BERT](https://huggingface.co/unitary/toxic-bert/tree/main). Напишем для этого функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(comments):\n",
    "    ## Токенизируем тексты комментариев\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('unitary/toxic-bert', max_length=512)\n",
    "    tokenized = comments.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "    \n",
    "    ## Приведем все векторы токенов к одному размеру\n",
    "    max_len = 512\n",
    "    padded = np.array([i + [0]*(max_len - len(i)) if len(i)<512  else i[:512]  for i in tokenized.values])\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "    print(f\"tokenized shape: {tokenized.shape}\")\n",
    "    print(f\"padded shape: {padded.shape}\")\n",
    "    print(f\"attention_mask shape: {attention_mask.shape}\")\n",
    "    \n",
    "    ## Инициализируем модель и запустим ее на cuda\n",
    "    config = config = transformers.BertConfig.from_pretrained('unitary/toxic-bert')\n",
    "    model = transformers.BertModel.from_pretrained('unitary/toxic-bert', config=config).cuda()\n",
    "    \n",
    "    ## Будем создавать эмбеддинги батчами по 20 текстов. Размер батча определен эмпирически.\n",
    "    batch_size = 20\n",
    "    embeddings = []\n",
    "    for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.cuda.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.cuda.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "        \n",
    "    return np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим векторное представление текстов отдельно для обучающей и тестовой выборок, чтобы не возникло эффекта подглядывания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1279 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized shape: (800,)\n",
      "padded shape: (800, 512)\n",
      "attention_mask shape: (800, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd41131fdfdc42ebac54d1e749224bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (800, 768)\n"
     ]
    }
   ],
   "source": [
    "X_train = get_features(comments_train)\n",
    "print(f\"features shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized shape: (200,)\n",
      "padded shape: (200, 512)\n",
      "attention_mask shape: (200, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5492b2d3b8452695491052d35b16eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (200, 768)\n"
     ]
    }
   ],
   "source": [
    "X_test = get_features(comments_test)\n",
    "print(f\"features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим несколько моделей: логистическую регрессию, случайный лес, LightGBM, CatBoost. Дисбаланс классов учтем с помощью весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.959830866807611\n",
      "CPU times: total: 1.7 s\n",
      "Wall time: 295 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced', random_state=42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(random_state=RANDOM_STATE, class_weight='balanced')\n",
    "print('F1:', cross_val_score(logreg, X_train, y_train, scoring='f1', cv=4).mean())\n",
    "\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(estimator, param_grid, X_train, y_train):\n",
    "    model = GridSearchCV(estimator=estimator, \n",
    "                            param_grid=param_grid, \n",
    "                            cv=3,\n",
    "                            scoring='f1')\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    best_index = model.best_index_\n",
    "    best_rmse = round(model.cv_results_['mean_test_score'][best_index], 4)\n",
    "\n",
    "    print(f\"Best F1: {best_rmse}\")\n",
    "    print(f\"Best params: {model.best_params_}\")\n",
    "\n",
    "    return model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1: 0.96\n",
      "Best params: {'max_depth': 2, 'n_estimators': 90}\n",
      "CPU times: total: 11.6 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_estimator = RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced')\n",
    "\n",
    "rf_param_grid =  {\n",
    "    'n_estimators': list(range(60, 121, 30)),\n",
    "    'max_depth': list(range(2, 13, 5)),\n",
    "}\n",
    "\n",
    "rf_best_model = fit_model(\n",
    "    estimator=rf_estimator,\n",
    "    param_grid=rf_param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1: 0.9772\n",
      "Best params: {'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 25}\n",
      "CPU times: total: 5min 19s\n",
      "Wall time: 53.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgbm_estimator = LGBMClassifier(random_state=RANDOM_STATE, class_weight='balanced', verbose=-1)\n",
    "\n",
    "lgbm_param_grid = {\n",
    "    \"n_estimators\": range(25, 101, 25), \n",
    "    \"max_depth\": range(5, 16, 5),\n",
    "    'learning_rate': [0.15, 0.2, 0.25]\n",
    "}\n",
    "\n",
    "lgbm_best_model = fit_model(\n",
    "    estimator=lgbm_estimator,\n",
    "    param_grid=lgbm_param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1: 0.955\n",
      "Best params: {'depth': 6, 'iterations': 100, 'learning_rate': 0.1}\n",
      "CPU times: total: 1h 37min 10s\n",
      "Wall time: 9min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "catboost_estimator = CatBoostClassifier(random_state=RANDOM_STATE, verbose=False, class_weights=class_weights)\n",
    "\n",
    "\n",
    "catboost_param_grid = {\n",
    "    \"iterations\": range(100, 201, 50),\n",
    "    \"learning_rate\": [0.1, 0.15, 0.2],\n",
    "    \"depth\": range(2, 7, 2)\n",
    "}\n",
    "\n",
    "catboost_best_model = fit_model(\n",
    "    estimator=catboost_estimator,\n",
    "    param_grid=catboost_param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве лучшей модели выберем LGBMClassifier, так как метрика F1 у нее лучше остальных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование\n",
    "\n",
    "Проверим качесто выбранной модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на тестовой выборке: 0.9545\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 на тестовой выборке: {f1_score(y_test, lgbm_best_model.predict(X_test)):.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество лучше требуемого F1 = 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Целью проекта была разработка модели для определения токсичности комментариев.\n",
    "\n",
    "В нашем распоряжении был датасет с разметкой о токсичности, который содержал 159 292 текстовых комментария. С целью ускорения расчетов было решено взять для обучения модели только часть данных - 1000 комментариев.\n",
    "\n",
    "Тексты были переведены в эмбеддинги с помощью модели BERT.\n",
    "\n",
    "На эмбеддингах были обучены четыре модели: логистическая регрессия, случайный лес, LightGBM и CatBoost. Среди этих моделей по качеству лучшей оказалась модель **LGBMClassifier** с параметрами: learning_rate=0.2, max_depth=5, n_estimators=50. На тестовой выборке лучшая модель показала **F1 = 0.95**, что лучше требуемого F1 = 0.75."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
